{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import custom modules\n",
    "from dataset import CustomDataset, PadCollate\n",
    "from model.optimizer import Ralamb, WarmupLinearSchedule\n",
    "from model.model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Parsing Method')\n",
    "parser.add_argument('--preprocessing', action='store_true')\n",
    "parser.add_argument('--training', action='store_true')\n",
    "# Path setting\n",
    "parser.add_argument('--data_path', default='/HDD/dataset/korean-hate-speech-detection/', type=str,\n",
    "                    help='Original data path')\n",
    "parser.add_argument('--save_path', default='./preprocessing', type=str,\n",
    "                    help='Preprocessed data & Model checkpoint file path')\n",
    "# Preprocessing setting\n",
    "parser.add_argument('--vocab_size', default=24000, type=int, help='Vocabulary size; Default is 24000')\n",
    "parser.add_argument('--pad_idx', default=0, type=int, help='pad index')\n",
    "parser.add_argument('--bos_idx', default=1, type=int, help='index of bos token')\n",
    "parser.add_argument('--eos_idx', default=2, type=int, help='index of eos token')\n",
    "parser.add_argument('--unk_idx', default=3, type=int, help='index of unk token')\n",
    "parser.add_argument('--max_len', default=150, type=int, help='Max Length of Source Sentence; Default is 150')\n",
    "# Model setting\n",
    "parser.add_argument('--d_model', default=768, type=int, help='Model dimension; Default is 768')\n",
    "parser.add_argument('--d_embedding', default=256, type=int, help='Embedding dimension; Default is 256')\n",
    "parser.add_argument('--n_head', default=12, type=int, help='Mutlihead count; Default is 12')\n",
    "parser.add_argument('--dim_feedforward', default=2048, type=int, help='Feedforward layer dimension; Default is 2048')\n",
    "parser.add_argument('--n_layers', default=12, type=int, help='Layer count; Default is 12')\n",
    "# Training setting\n",
    "parser.add_argument('--num_epochs', default=30, type=int, help='Epoch count; Default is 30')\n",
    "parser.add_argument('--batch_size', default=16, type=int, help='Batch size; Default is 16')\n",
    "parser.add_argument('--dropout', default=0.3, type=float, help='Dropout ratio; Default is 0.3')\n",
    "parser.add_argument('--lr', default=1e-3, type=float, help='Learning rate; Default is 1e-3')\n",
    "parser.add_argument('--w_decay', default=5e-4, type=float, help='Weight decay ratio; Default is 5e-4')\n",
    "args = parser.parse_args(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load & Setting!\n",
      "Total number of trainingsets  iterations - 7896, 493\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#===================================#\n",
    "#============Data Load==============#\n",
    "#===================================#\n",
    "\n",
    "# 1) Data open\n",
    "print('Data Load & Setting!')\n",
    "with open(os.path.join(args.save_path, 'processed.pkl'), 'rb') as f:\n",
    "    data_ = pickle.load(f)\n",
    "    train_indices = data_['train_indices']\n",
    "    valid_indices = data_['valid_indices']\n",
    "    train_title_indices = data_['train_title_indices']\n",
    "    valid_title_indices = data_['valid_title_indices']\n",
    "    train_total_indices = data_['train_total_indices']\n",
    "    valid_total_indices = data_['valid_total_indices']\n",
    "    train_label = data_['train_label']\n",
    "    valid_label = data_['valid_label']\n",
    "    word2id = data_['word2id']\n",
    "    id2word = data_['id2word']\n",
    "    vocab_num = len(word2id.keys())\n",
    "    del data_\n",
    "\n",
    "dataset_dict = {\n",
    "    'train': CustomDataset(train_total_indices, train_indices, \n",
    "                        train_title_indices, train_label,\n",
    "                        max_len=args.max_len),\n",
    "    'valid': CustomDataset(valid_total_indices, valid_indices, \n",
    "                        valid_title_indices, valid_label,\n",
    "                        max_len=args.max_len),\n",
    "}\n",
    "dataloader_dict = {\n",
    "    'train': DataLoader(dataset_dict['train'], collate_fn=PadCollate(), drop_last=True,\n",
    "                        batch_size=16, shuffle=True, pin_memory=True,\n",
    "                        num_workers=2),\n",
    "    'valid': DataLoader(dataset_dict['valid'], collate_fn=PadCollate(), drop_last=True,\n",
    "                        batch_size=16, shuffle=True, pin_memory=True,\n",
    "                        num_workers=2)\n",
    "}\n",
    "print(f\"Total number of trainingsets  iterations - {len(dataset_dict['train'])}, {len(dataloader_dict['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (transformer_embedding): TransformerEmbedding(\n",
       "    (token): TokenEmbedding(12000, 256, padding_idx=0)\n",
       "    (linear_layer): Linear(in_features=256, out_features=768, bias=True)\n",
       "    (position): PositionalEmbedding()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output_linear): Linear(in_features=768, out_features=256, bias=False)\n",
       "  (output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (output_linear2): Linear(in_features=256, out_features=3, bias=False)\n",
       "  (encoders): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (6): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (7): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (8): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (9): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (10): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (11): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Model initiating\n",
    "print(\"Instantiating models...\")\n",
    "model = Transformer(vocab_num=vocab_num, pad_idx=args.pad_idx, bos_idx=args.bos_idx, \n",
    "                    eos_idx=args.eos_idx, max_len=args.max_len, d_model=args.d_model, \n",
    "                    d_embedding=args.d_embedding, n_head=args.n_head, \n",
    "                    dim_feedforward=args.dim_feedforward, n_layers=args.n_layers, \n",
    "                    dropout=args.dropout, device=device)\n",
    "optimizer = Ralamb(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=args.lr, weight_decay=args.w_decay)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=len(dataloader_dict['train'])*3, \n",
    "                                t_total=len(dataloader_dict['train'])*args.num_epochs)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=args.pad_idx)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "# 1) Pre-setting\n",
    "best_val_loss = None\n",
    "\n",
    "# 2) Training start\n",
    "for e in range(start_epoch, args.num_epochs):\n",
    "    start_time_e = time.time()\n",
    "    for phase in ['train', 'valid']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        if phase == 'valid':\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "        for i, (total, comment, title, label) in enumerate(dataloader_dict[phase]):\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source, Target  setting\n",
    "total = total.to(device)\n",
    "comment = comment.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "# Optimizer setting\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / Calculate loss\n",
    "with torch.set_grad_enabled(phase == 'train'):\n",
    "    output = model(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9738,  0.3198, -0.3971],\n",
       "        [ 0.4141,  0.6645,  0.1555],\n",
       "        [-0.1024,  1.0948, -0.9971],\n",
       "        [ 0.2354,  0.3047,  0.6112],\n",
       "        [-0.6235, -0.3289, -0.7982],\n",
       "        [ 0.9417, -0.1104,  0.0428],\n",
       "        [ 0.2064,  0.8898, -0.4838],\n",
       "        [-0.1077,  1.0312,  0.0220],\n",
       "        [ 0.2509,  0.3141, -0.3987],\n",
       "        [ 0.8201,  0.1591, -1.0772],\n",
       "        [ 0.4695,  1.1079, -0.4658],\n",
       "        [-0.2693,  0.4440, -0.8266],\n",
       "        [ 0.8681,  0.2959, -0.6716],\n",
       "        [ 0.6637, -0.0824, -1.4008],\n",
       "        [ 0.0335,  0.6816, -0.5836],\n",
       "        [-0.1174,  0.2077, -1.4999]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2, 2, 1, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output[:,0], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2599, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3125"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(output[:,0].max(dim=1)[1] == label).item() / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(output[:,0].max(dim=1)[1] == label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
